---
title: "Understanding Word Embedding"
author: "Arjun Paudel"
date: "6/7/2021"
output: 
  github_document:
    pandoc_args: --webtex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidytext)
library(textdata)
library(widyr)
library(irlba)
```

```{r}
dt <- tibble(id = c(1,2,1,2),
             text = c("This is a good dog",
                      "Who is a good boy",
                      "Boy, it sure is hot today, huh?",
                      "This is a hot summer month")
             )
dt <- dt %>% arrange(id)
dt
```

We first tokenize the text and format it in a tidy fashion, i.e. one token per row.    
`unnest_tokens` from **tidytext** is the go to function for this.
```{r}
tidy_dt <- dt %>% 
  unnest_tokens(word, text)
tidy_dt
```

We also create bi-grams and calculate pointwise mutual information(pmi) using `pairwise_pmi()` from **widyr** package.
```{r}
tidy_ngram_dt <- dt %>% 
  unnest_tokens(words, text, token = "ngrams", n = 2, collapse = "id") %>% 
  mutate(.ngramID = row_number()) %>% 
  unnest_tokens(word, words) %>% 
  unite(.ngramID, id, .ngramID)

tidy_pmi <- tidy_ngram_dt %>% 
  pairwise_pmi(item = word, feature = .ngramID, sort = TRUE)
tidy_pmi
```

> Notice the use of `collapse = "id"`
>
> Without this, the bigram tokenizer will stop at the end of each row and will not crossover to next row.
>
> With `collapse = "id"` it will cross over to next row with same id and treat last word of previous row and first word of next row together.
>
> In our case, row1 and row3 have same ***id***, so we can think of this as one single long sentence that we just chose to represent in two rows. `collapse = "id"` will treat it such.
>
> Sometimes we read a big text file, certain number of lines at a time. So all the text of same document can end up in different rows. They will share the same document id but not row number. Using `collapse = "document_id"` will treat all those different rows as part of same document.

Implementation from [smltar](https://smltar.com/embeddings.html)    
The only difference is how the skipgrams are calculated.

```{r}

nested_words <- tidy_dt %>%
  nest(words = c(word))

nested_words


slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl, 
    ~.x, 
    .after = window_size - 1, 
    .step = 1, 
    .complete = TRUE
  )
  
  safe_mutate <- safely(mutate)
  
  out <- map2(skipgrams,
              1:length(skipgrams),
              ~ safe_mutate(.x, window_id = .y))

  out %>%
    transpose() %>%
    pluck("result") %>%
    compact() %>%
    bind_rows()
}


tidy_pmi2 <- nested_words %>%
  mutate(words = map(words, slide_windows, 2L)) %>%
  unnest(words) %>%
  unite(window_id, id, window_id) %>%
  pairwise_pmi(word, window_id, sort = TRUE)

tidy_pmi2

```

PMI calculated by both methods is identical.

```{r}
identical(tidy_pmi, tidy_pmi2)
```

**Manually calculating PMI**

There seems to be different definition and methods of calculating PMI. I found this [paper](https://proceedings.neurips.cc/paper/2009/file/185c29dc24325934ee377cfda20e414c-Paper.pdf) which shows how PMI is calculated and `pairwise_pmi` from **widyr** seems to follow this method.    
We will manually calculate pmi and compare that to `pairwise_pmi`.

```{r}
dat <- tibble(group = rep(1:5, each = 2),
              letter = c("a", "b",
                         "a", "c",
                         "a", "c",
                         "b", "e",
                         "b", "f"))
dat %>% pairwise_pmi(letter, group)

```
There are 5 total tokens, out of that "ac" shows up on two tokens. "a shows up on 3 tokens and "c shows up on 2 tokens. So,        
$I(a,c) = log(\frac{\frac{2}{5}}{\frac{3}{5}*\frac{2}{5}}) =$ `r log((2/5)/((3/5)*(2/5)))`    

There are total 5 tokens. "ab" appears in 1 token. "a" appears in 3 tokens and "b" appears in 3 tokens.        
$I(a,c) = log(\frac{\frac{1}{5}}{\frac{3}{5}*\frac{3}{5}}) =$ `r log((1/5)/((3/5)*(3/5)))`    
Both of these results match with what we got from `pairwise_pmi()`.

### Represent words as numeric vectors    
Now we have a very high dimensional word-word matrix with associated pmi values. We can reduce the dimension of this matrix using single value decomposition.    

> Really good [youtube video](https://www.youtube.com/watch?v=DG7YTlGnCEo) explaining SVD and its application in image compression.

Create 5 dimensional vector using `widely_svd()` from **widyr** package

```{r}
set.seed(13)
tidy_word_vectors <- tidy_pmi %>% 
  widely_svd(
    item1, item2, pmi,
    nv = 5, maxit = 10000
  )
```

We can try to recreate this using `irlba()` from **irlba** library

```{r}
pmi_matrix <- tidy_pmi %>% 
  cast_sparse(item1, item2, pmi)

class(pmi_matrix)

set.seed(13)
pmi_svd <- irlba(pmi_matrix, nv=5, maxit = 10000)
pmi_svd

word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)

```

Comparing results obtained from `widely_svd()` and `irlba()`.
```{r}
word_vectors
tidy_word_vectors %>% cast_sparse(item1, dimension, value)
```

```{r}
word_vectors == tidy_word_vectors %>% cast_sparse(item1, dimension, value) 
```

>Initially i got similar but not exact same results from the two methods above. Third dimension in particular was very different and there were some sign discrepencies. I was not sure if this was due to rounding error (values in 3rd dimensions are really small -- at the order of $10^-01 - 10^-16$) or different implementation of SVD.

> Looking at source code of `widely_svd()`, i found that it calls `irba()`, so I was really confused why i was not getting same results.

> Later realized that the inconistencies were due to random nature of the process. Both svd processes resulted in identical result after setting same seed before function calls.

#### Similarity

Similarity between vector representation of words can be found using cosine similarity.        
$sim(x,y) = \frac{x.y}{||x||.||y||}$

$x = (5, 0, 3, 0, 2, 0, 0, 2, 0, 0)$ and  $y = (3, 0, 2, 0, 1, 1, 0, 1, 0, 1)$     
$x.y = x^{t}.y = 5*3+0*0+3*2+0*0+2*1+0*1+0*0+0*1 = 25$          
$||x|| = \sqrt{5^2+0^2+3^2+0^2+2^2+0^2+0^2+2^2+0^2+0^2} = 6.48$         
$||y|| = \sqrt{3^2+0^2+2^2+0^2+1^2+1^2+0^2+1^2+0^2+1^2} = 4.12$            
$sim(x,y) = \frac{25}{6.48*4.12} = 0.94$        


**Some matrix multiplication quirks to understand.**

Let's create a 2x2 matrix.
```{r}
a <- matrix(c(1,2,1,2), nrow = 2, byrow = TRUE)
a
```

If we slice one row or one column, the result is just a vector not a matrix.
```{r}
a[1,]
a[,1]
```

Matrix have dimensions but the dimension of a vector is `NULL`
```{r}
a %>% dim()
a[1,] %>% dim()
a[,1] %>% dim()
```

Since the dimension of an vector is NULL it can act as a row vector or column vector.    
In this case it acts as a 1x2 row vector and so can be multiplied by a 2x2 matrix.    
$a^*_{1\times2}*a_{2\times2}$
```{r}
a[1,]%*%a
```

Here it acts as 2x1 column vector and can multiply a.         
$a_{2\times2}*a^*_{2\times1}$
```{r}
a%*%a[1,]
```

Outcome of both above operations result in a matrix of appropriate dimensions.

```{r}
nearest_neighbors <- function(df, token) {
  df %>%
    widely(
      ~ {
        y <- (. %*% (.[token, ]))[,1]
        res <- y / 
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
      matrix(res, ncol = 1, dimnames = list(x = names(res)))
      },
      sort = TRUE
    )(item1, dimension, value) %>% 
  select(-item2)
}
```

Implementation from the book.
```{r}
nearest_neighbors2 <- function(df, token) {
  df %>%
    widely(
      ~ {
        y <- .[rep(token, nrow(.)), ]
        res <- rowSums(. * y) / 
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))

        matrix(res, ncol = 1, dimnames = list(x = names(res)))
        },
      sort = TRUE
    )(item1, dimension, value) %>%
    select(-item2)
}
```

Identical results
```{r}
tidy_word_vectors %>% nearest_neighbors2("today")
tidy_word_vectors %>% nearest_neighbors("today")
```

#### Word embedding

Now we have documents made up of multiple words and we know the vector representation of those words. So how do we go about representing the documents in vector form.    
One common approach is to aggregate the word vectors of all the words that make up the document. We could sum all those word vectors of a document or average them or take the min/max across each dimension.    

We need a matrix of *document_id*, *words* and the count of how many times that word appears in that document.

Document-term(word) matrix
```{r}
tidy_dt %>% head()
doc_term_matrix <- tidy_dt %>% 
  count(id, word) %>% 
  cast_sparse(id, word, n)
```

Word embedding matrix
```{r}
tidy_word_vectors %>% head()
embedding_matrix <- tidy_word_vectors %>%
  cast_sparse(item1, dimension, value)
embedding_matrix
```

Document matrix
```{r}
doc_term_matrix %*% embedding_matrix 
```

**Manually calculating document matrix**     

Using matrix multiplication
```{r}
test_dtm <- matrix(c(1,2,0,2,0,1), nrow = 2)
dimnames(test_dtm) <- list(c(1,2), c("the", "good", "boy"))
test_dtm
test_embed <- matrix(c(.2,.4,.1,1,2,3,.5,.25,.4, .02,.15,.8), nrow = 3)
dimnames(test_embed) <- list(c("the", "good", "boy"), c("d1", "d2", "d3", "d4"))
test_embed

doc_matrix <- test_dtm %*% test_embed
doc_matrix
```

We can replicate this using simple count, sum and multiplication in tidy format.

```{r}
tidy_embed<-test_embed %>% as_tibble(rownames = "word") %>% 
  pivot_longer(!word, names_to = "dimension", values_to = "value")

tidy_dtm <- tibble(word = rep(c("the", "good", "boy"), times = 2),
                   id = rep(c(1,2), each = 3,),
                   n = c(1,0,0,2,2,1)
)

tidy_doc_matrix <- tidy_dtm %>% left_join(tidy_embed, by = "word") %>% 
  mutate(value = n*value) %>% 
  pivot_wider(names_from = dimension, values_from = value,values_fill = 0) %>% 
  group_by(id) %>% 
  summarise(across (starts_with("d"), sum)) 

```

Both results are identical

```{r}
doc_matrix
tidy_doc_matrix

```

If you want *tidy_doc_matrix* in matrix format rather than a tibble, use `cast_sparse`.
```{r}
tidy_doc_matrix %>% 
  pivot_longer(starts_with("d"), names_to = "dimension", values_to = "value") %>% 
  cast_sparse(id, dimension, value)
doc_matrix
```

#### Pretrained word Embedding

We will use `embedding_glove6b()` function from **tidytext** package to access to 6B tokens glove embedding from the Stanford NLP Group.

> First time you use this function, you will get a prompt to accpet license and such.
> It will also take some time to download the dataset of word-vector to your disk.
> After first use, everytime you use the function, the word-vector dataset will be loaded from disk.

Load 50 dimensional word-vector.
```{r}
glove6b <- textdata::embedding_glove6b(dimensions = 50)
```

Use `step_word_embedding` from  **textrecipes** 
```{r}

dt2 <- dt %>% mutate(id = c(1,2,3,4))
rec_spec <- recipes::recipe(~. , data = dt2) %>% 
  textrecipes::step_tokenize(text) %>% 
  textrecipes::step_word_embeddings(text, embeddings = glove6b) %>% 
  recipes::prep()

dt_baked <- rec_spec %>% recipes::bake(new_data = NULL)
dt_baked[, 1:6]

```

**Manual steps**
```{r}
tidy_glove <- glove6b %>%
  pivot_longer(contains("d"),
               names_to = "dimension") %>%
  rename(item1 = token)

tidy_dt2 <- dt %>% 
  mutate(id = c(1,2,3,4)) %>% 
  unnest_tokens(word, text)


word_matrix <- tidy_dt2 %>% 
  inner_join(by = "word",
             tidy_glove %>%
               distinct(item1) %>%
               rename(word = item1)) %>%
  count(id, word) %>% 
  arrange(word) %>% 
  cast_sparse(id, word, n)

glove_matrix <- tidy_glove %>% 
  inner_join(by = "item1",
             tidy_dt2 %>%
               distinct(word) %>%
               rename(item1 = word)) %>% 
  arrange(item1) %>% 
  cast_sparse(item1, dimension, value)


doc_matrix <- word_matrix %*% glove_matrix
doc_matrix[,1:5] 

```

> It is extremely important that the order of column in word_matrix and order of row in glove_matrix match. If there is a mismatch you will get incorrect result.    
> In our case this is achieved by sorting dataframe by word/token before casting to sparse matrix.

**Another way of calculating document embedding.**    
Assuming each word only appears once in a document, we can simply filter the gove word-vector dataset to the desired tokens and sum the values across each dimension.
```{r}
tok_1 <- c("this","is", "a", "good", "dog")
tok_2 <- c("who","is", "a", "good","boy")
tok_3 <- c("boy","it", "sure", "is", "hot", "today", "huh" )
tok_4 <- c("this", "is", "a", "hot", "summer", "month" )
glove6b %>% select(token, d1:d5) %>% 
  filter(token %in% tok_1) %>% 
  select(where(is.numeric)) %>% 
  map_df(sum)
```

